\documentclass{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{subfigure}

\usepackage[T1]{fontenc}
\RequirePackage{float} % um tabellen/grafiken mit [H]genau dahin zu plotten wo sie hingehren


\sloppy
\newcommand{\transp}{^{\sf T}}
\newcommand{\x}{\vec{x}}
\newcommand{\w}{\vec{w}}
\newcommand{\p}{\vec{p}}
\newcommand{\va}{\vec{a}}
\newcommand{\vatilt}{\vec{\bar{a}}}
\newcommand{\btilt}{\bar{b}}
\newcommand{\q}{\vec{q}}
\newcommand{\vu}{\vec{u}}
\newcommand{\vc}{\vec{c}}
\newcommand{\vv}{\vec{v}}
\newcommand{\valpha}{\vec{\alpha}}
\newcommand{\vbeta}{\vec{\beta}}
\newcommand{\vgamma}{\vec{\gamma}}
\newcommand{\vsigma}{\vec{\sigma}}
\newcommand{\vd}{\vec{d}}
\newcommand{\vb}{\vec{b}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vxi}{\vec{\xi}}
\newcommand{\veps}{\vec{\epsilon}}
\newcommand{\unit}{\vec{e}}
\newcommand{\sign}{\text{sign}}
\newcommand{\Qtilt}{\text{\~{Q}}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\begin{document}

\title{\emph{SoTool(HmSVM-Toolbox 0.3)}: A Structured Output Prediction Toolbox}
\author{Documention by Nico G\"ornitz}
\date{\today}
\maketitle              % typeset the title of the contribution

\tableofcontents

\section{Piecewise Linear Functions (PLiFs)}

PLiFs are used to transform and bound the first-stage SVM outputs. The number of supporting points is set in advance (see main.m, PAR.num\_plif\_nodes=20). Scr.limits are set in advance by measuring signal densities: high density regions have more supporting points than low density regions (see init\_parameters.m). An easier approach would be to distribute them equidistantly. 
 
\begin{figure}[H]
\includegraphics[scale=0.61]{figures/PlifExample2} 
\end{figure}

If 'Input' is smaller than the first 'limit' point it takes it's corresponding 'score' value no matter how much smaller 'Input' is. 
Why PLiFs? We need to calculate $\w^T \delta \Psi_i(\bar{y}) = \w^T \{ \Psi(\x_i,y_i)-\Psi(\x_i,\bar{y}) \}$. Therefore the term must be linear decomposable.



\section{Hidden Markov Property and Viterbi-like \emph{argmax}-calculation}

\subsection{Why \emph{HM}SVM}

In HMM we model the joint probability of two sequences $\vy$ and $\x$ as $p(\vy,\x) = \prod_n p(y_n|y_{n-1}) p(x_n|y_n)$.


\subsection{Computing the Emission-scores}

\subsection{Calculating the State-sequence}




\section{Optimization Problem}
SoSVM

\begin{align*}
\min_{\w,\vxi \geq 0} \quad & \frac{1}{2} \Vert \w \Vert_2^2 + C \sum_i \xi_i \\
\text{subject to} \quad & \langle\w, \Psi(\x_i,y_i) \rangle - \langle \w, \Psi(\x_i,\bar{y}) \rangle \geq \Delta(y_i,\bar{y})-\xi_i,  \quad \forall i,\bar{y} \neq y_i 
\end{align*}


Full optimization problem:
\begin{align*}
\min_{\w,\veps,\vxi} \quad & \frac{1}{2} \w^T Q \w + \frac{1}{2}C_{\text{s}}\sum_m \epsilon_m^2 + \frac{1}{2}C_{\text{c}}\sum_n \epsilon^2_n + \sum_i \xi_i \\
\text{subject to} \quad & \w^T \delta \Psi_i(\bar{y}) \geq \Delta(y_i,\bar{y})-\xi_i,  \xi_i \geq 0 \quad \forall i, \bar{y} \neq y_i  \\
& |\w^T \vu_m| \leq \epsilon_m, |\w^T \vu_n| \leq \epsilon_n \quad \text{and} \quad \w^T \vu_l \leq 0 \quad \forall l,m,n
\end{align*}
Reduced optimization problem:
\begin{align*}
\min_{\w} \quad & \frac{1}{2} \w^T \Qtilt \w + \sum_i \max\{0,\max_{\bar{y}\neq y_i}\Delta(y_i,\bar{y})-\w^T \delta \Psi_i(\bar{y})\} \\
\text{subject to} \quad & \w^T \vu_l \leq 0 \quad \forall l
\end{align*}

Where $\Qtilt = Q + S + K$.\\
Since $|\w^T \vu_m| \leq \epsilon_m \longrightarrow C_{\text{s}}\sum_m \epsilon^2 = C_{\text{s}}\sum_m \max\{-w^T \vu_m,w^T \vu_m\}^2$,
that is $ \w^T (C_{\text{s}} \sum_m \vu_m \vu^T_m ) \w = \w^T S \w$. Same goes for the matrix K (Coupling constraints).


\subsection{Approximation I: Relaxing Monotonicity Constraints}
Don't do this.\\
Simplify model constraints $\w^T \vu_l \leq 0$ with a hinge loss or quadratic hinge loss yields an approximate unconstraint optimization problem (with additional trade-off parameter $C_l$):
\begin{align*}
\min_{\w} \quad & \frac{1}{2} \w^T \Qtilt \w + C_l \sum_l \max(0,\w^T \vu_l)+
\sum_i \max\{0,\max_{\bar{y}\neq y_i}\Delta(y_i,\bar{y})-\w^T \delta \Psi_i(\bar{y})\}
\end{align*}
The above problem turned out to be \emph{significantly} slower in practice (Especially when using bundle methods for regularized risk minimization).

\subsection{Approximation II: Backprojection}
Don't do this.\\
Idea: use the unconstraint optimization method (by ignoring the monotonicity constraints)
yields
\begin{align*}
\w^* = \argmin_{\w} \quad & \frac{1}{2} \w^T \Qtilt \w + \sum_i \max\{0,\max_{\bar{y}\neq y_i}\Delta(y_i,\bar{y})-\w^T \delta \Psi_i(\bar{y})\} 
\end{align*}
To ensure that monotonicity is fullfilled project the solution $w^*$ onto the valid subspace:
\begin{align*}
\min_{\w} \quad & \frac{1}{2} ||\w^*-\w||^2_2 = \min_{\w} \frac{1}{2} \w^T\w - \w^T\w^*\\
\text{subject to} \quad & \w^T \vu_l \leq 0 \quad \forall l
\end{align*}
Problem: practically the solution might oscillate extremly and therefore not converge fast (if ever).
Not checked solution: using 1-norm instead of 2-norm distance.





\section{Convex Bundle Methods}
\begin{align*}
\min_{\w} \quad & \frac{1}{2} \w^T \Qtilt \w + \max_i \{\va_i^T \w + b_i\}\\
\text{subject to} \quad & \w^T \vu_l \leq 0 \quad \forall l
\end{align*}
is equivalent to
\begin{align}\label{cbm-primal}
\min_{\w,\xi} \quad & \frac{1}{2} \w^T \Qtilt \w + \xi \nonumber\\
\text{subject to} \quad & \va_i^T \w + b_i \leq \xi \quad \forall i\\
& \w^T \vu_l \leq 0 \quad \forall l \nonumber
\end{align}



\subsection{Convex Hull Subgradient Calculation}


\subsection{Solving the Dual Problem}
Here, we derive the dual of the problem stated in Equation \eqref{cbm-primal}. The Lagrange-function looks like:
\begin{align*}
L(\w,\xi,\valpha,\vbeta) =& \frac{1}{2} \w^T \Qtilt \w + \xi 
+ \sum_i \alpha_i (\va_i^T \w + b_i - \xi) + \sum_l \beta_l \w^T \vu_l \\
\text{subject to} \quad & \alpha_i \geq 0 \quad \forall i\\
\quad & \beta_l \geq 0 \quad \forall l
\end{align*}
Setting the derivations with respect to the primal variables zero, yield:
\begin{align*}
\partial_{\w} L &=^! 0 \rightarrow \w = -\Qtilt^{-1} (\sum_i \alpha_i \va_i + \sum_l \beta_l \vu_l) = -\Qtilt^{-1} \underbrace{(\valpha^T A + \vbeta^T U)}_{\vgamma}\\
\partial_\xi L &=^! 0 \rightarrow \sum_i \alpha_i = 1
\end{align*}
Dual problem arrives at
\begin{align*}
\max_{\vgamma} \quad & -\frac{1}{2} \vgamma^T \Qtilt^{-1,T} \vgamma + \valpha^T \vb  = \frac{1}{2} \left[{\valpha \atop \vbeta} \right]^T \left[{A \atop U} \right] \Qtilt^{-1,T} \left[{A \atop U} \right]^T \left[{\valpha \atop \vbeta} \right]	 \\
\text{subject to} \quad & \sum_i \alpha_i = 1, \quad \alpha_i \geq 0, \quad \beta_l \geq 0 \quad \forall i,l
\end{align*}
and can be expressed as
\begin{align*}
\min_{\vsigma} \quad & \frac{1}{2} \vsigma^T Z \vsigma - \vsigma^T \vb' \\
\text{subject to} \quad & \vsigma^T \vc = 1, \quad \sigma_i \geq 0, \quad \forall i
\end{align*}
with $\vsigma = \left[{\valpha \atop \vbeta} \right]$, $Z = \left[{A \atop U} \right] \Qtilt^{-1,T} \left[{A \atop U} \right]^T$, $\vc = \left(1,\dots,1,0,\dots0\right)^T$ and $\vb' = \left(b_1, \dots, b_{\#\valpha},0,\dots0\right)^T$

\subsubsection{Integrated Approach: Triple-SMO}
Idea: Select two variables $\alpha_k$ and $\alpha_l$ which are bound by the equality constraint (Smo--approach) and an additional $\beta_m$ which doesn't depend on the equality constraint. Express the optimal $\beta_m$ in terms of $\alpha$s (set derivation to zero). Attention: $\beta_m$ is constrained to be $\geq 0$.

Rewrite the optimization problem in terms of $\beta$'s which are only bound to the inequality constraint and occur in the quadratic part of the objective function and $\alpha$'s which are additionally bound to the equality constraint and also occur in the linear part of the objective:

\begin{align*}
\min_{\valpha,\vbeta} \quad & \frac{1}{2} \valpha^T A \valpha + \valpha^T B \vbeta + \frac{1}{2} \vbeta^T C \vbeta - \valpha^T \vb \\
\text{subject to} \quad & \valpha^T \vc = 1, \quad \alpha_i \geq 0, \quad \beta_j \geq 0, \quad \forall i,j
\end{align*}

Split some $\alpha_l$, $\alpha_k$ and $\beta_m$:
\begin{align*}
\valpha^T A \valpha &= 2 \alpha_k v_k + 2 \alpha_l v_l + \alpha_k^2 A_{kk} + \alpha_l^2 A_{ll} + 2 \alpha_k \alpha_l A_{kl} + K_A\\
\valpha^T B \vbeta &= \beta_m h_m + \alpha_k h_k + \alpha_l h_l + \alpha_k \beta_m B_{km} + \alpha_l \beta_m B_{lm} + K_B \\
\vbeta^T C \vbeta &= 2 \beta_m w_m + \beta_m^2 C_{mm} + K_C\\
\valpha^T \vb &= \alpha_k b_k + \alpha_l b_l + K_L
\end{align*}
where 
\begin{align*}
v_i = \sum_{j \neq i} A_{ij} \alpha_j \quad 
w_i = \sum_{j \neq i} C_{ij} \beta_j \quad
h_i = \sum_{j \neq i} B_{ij} \beta_j 
\end{align*}
define
\begin{align*}
u_i = \sum_{j} A_{ij} \alpha_j = v_i + \alpha_k A_{ik} + \alpha_l A_{il}
\end{align*}

Due to the equality constraints $\alpha_k + \alpha_l = d$. Replace $\alpha_k = (d-\alpha_l)$:
\begin{align*}
\valpha^T A \valpha &= 2 (d-\alpha_l) v_k + 2 \alpha_l v_l + (d-\alpha_l)^2 A_{kk} + \alpha_l^2 A_{ll} + 2 (d-\alpha_l) \alpha_l A_{kl} + K_A\\
\valpha^T B \vbeta &= \beta_m h_m + (d-\alpha_l) h_k + \alpha_l h_l + (d-\alpha_l) \beta_m B_{km} + \alpha_l \beta_m B_{lm} + K_B \\
\valpha^T \vb &= (d-\alpha_l) b_k + \alpha_l b_l + K_L
\end{align*}
which is
\begin{align*}
\valpha^T A \valpha &= 2 d v_k - 2 \alpha_l v_k + 2 \alpha_l v_l + d^2 A_{kk}- 2 d \alpha_l A_{kk} + \alpha_l^2 A_{kk} + \alpha_l^2 A_{ll} + 2 d \alpha_l A_{kl} - 2 \alpha_l^2 A_{kl} + K_A\\
\valpha^T B \vbeta &= \beta_m h_m + d h_k -\alpha_l h_k + \alpha_l h_l + d \beta_m B_{km} - \alpha_l \beta_m B_{km} + \alpha_l \beta_m B_{lm} + K_B \\
\valpha^T \vb &= d b_k -\alpha_l b_k + \alpha_l b_l + K_L
\end{align*}

Derivatives
\begin{align*}
\partial_{\alpha_l} \valpha^T A \valpha &= - 2 v_k + 2 v_l - 2 d A_{kk} + 2 \alpha_l A_{kk} + 2 \alpha_l A_{ll} + 2 d A_{kl} - 4 \alpha_l A_{kl}\\
\partial_{\alpha_l} \valpha^T B \vbeta &= -h_k + h_l - \beta_m B_{km} + \beta_m B_{lm} \\
\partial_{\alpha_l} \valpha^T \vb &= -b_k + b_l
\end{align*}

With $L = \frac{1}{2} \valpha^T A \valpha + \valpha^T B \vbeta - \valpha^T \vb$ and $\partial_{\alpha_l} L= 0$:

\begin{align*}
0 = (v_l- v_k) + \alpha_l (\underbrace{A_{kk} + A_{ll} - 2 A_{kl}}_\tau) + d (A_{kl} - A_{kk}) + \underbrace{(h_l-h_k - \beta_m B_{km} + \beta_m B_{lm} + b_k - b_l)}_\rho
\end{align*}

Because current round variables $\alpha^-_k + \alpha^-_l = d$ too:
\begin{align*}
-(v_l- v_k) &= (u^-_k-u^-_l) + \alpha^-_l(A_{ll}-A_{kl}) + \alpha^-_k(A_{kl}-A_{kk})\\
-d (A_{kl} - A_{kk}) &= \alpha^-_l(A_{kk}-A_{kl}) - \alpha^-_k(A_{kl}-A_{kk})
\end{align*}

\begin{align*}
\alpha_l \tau &=  (u^-_k-u^-_l) + \alpha^-_l \tau - \rho \\
\alpha_l &=  \frac{(u^-_k-u^-_l)-\rho}{\tau} + \alpha^-_l \\
\end{align*}


\subsubsection{Other Approaches}
Interior Point Solver (\emph{PrLOQO})\\
Combined Dual Coordinate Descent and SMO: not possible because $Z$ is not a linear kernel. 

\subsection{Aggregation}
Idea: instead of adding more and more cutting planes it would be nice to use $K$ (constant) cutting planes but do not forget old cutting planes. Solution: calculate an \emph{aggregated} cutting plane $(\vatilt$,$\btilt)$ every iteration an use it in the next iteration
\begin{align*}
\min_{\w} \quad & \frac{1}{2} \w^T \Qtilt \w + \max[\max_i \{\va_i^T \w + b_i\}, \vatilt^T \w + \btilt ]\\
\text{subject to} \quad & \w^T \vu_l \leq 0 \quad \forall l.
\end{align*}

\begin{align*}
\vgamma^* = \argmax_{\vgamma} \quad & -\frac{1}{2} \vgamma^T \Qtilt^{-1,T} \vgamma + \valpha^T \vb\\
\text{subject to} \quad & \sum_i \alpha_i = 1, \quad \alpha_i \geq 0, \quad \beta_l \geq 0 \quad \forall i,l
\end{align*}



\subsection{Linesearch}
Because of the monotonicity constraints the linesearch also gets a bit harder to solve. Additionally, it is only applicable for structured output problems if no Viterbi is involved. This is done by optimizing the estimated empirical risk instead of the real empirical risk. Figure \ref{fig-linesearch} (left) shows a typical line from the bundle methods optimal $\w^*$ through the optimum of the last linesearch $\w_{ls}$ (real example). Figure \ref{fig-linesearch} (right) is an idea how the line was obtained (fake example). Direction $\vd = \w_{ls} - \w^*$. New points are obtained using the formular: $\w_{new} = \w^* + \lambda \vd$. Task for the linesearch is to estimate/calculate the $\lambda$.

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.49\textwidth]{figures/linesearch} 
\includegraphics[width=0.49\textwidth]{figures/linesearchBasic} 
\label{fig-linesearch}
\end{center}
\end{figure}


\subsubsection{Vanilla Linesearch}
$N$ points taken equidistantly between last linesearch point and and new bundle method solution ($0 \leq \lambda \leq 1$). Because last solution is a valid one, the new solution too and the feasible set is \emph{convex} all solutions found in between those two points are valid solutions as well. No further checking needed.


\subsubsection{Constraint Parabola Linesearch}
Idea: use 3 (\emph{scalar}) points $n_a$, $n_b$ and $n_c$ with corresponding 
objective function values $o_a$, $o_b$ and $o_c$ to find the new minimum at $\w_{new} = \w^* + n_{new} \vd $ . Fit a parabola through these
points 
\begin{align*}
f(x) &= p_1 x^2 + p_2 x + p_3\\
\text{subject to} \quad & \w^T \vu_l \leq 0 \quad \forall l
\end{align*}
with $f(n_a)=o_a$, $f(n_b) = o_b$ and $f(n_c) = o_c$ being valid solutions.
Calculate the new minima (at $f'(x) = 0$):
\begin{align*}
f'(x) & = 2 p_1 x + p_2 =^! 0 \rightarrow x = - \frac{p_2}{2p_1}.
\end{align*}
Task: find the parameters $p_1$, $p_2$ and $p_3$. It is simplier (less \emph{if}--statements) to
ensure that $o_c < o_b$ and $o_c < o_a$. The three starting points are $n_{a,b,c} \in \{0.0,0.5,1.0\}$. 
\[
\begin{pmatrix}
 n_a^2& n_a & 1\\ 
 n_b^2& n_b & 1\\ 
 n_c^2& n_c & 1 
\end{pmatrix} \cdot 
\begin{pmatrix}
 p_1\\ 
 p_2\\ 
 p_3 
\end{pmatrix}= 
\begin{pmatrix}
 o_a\\ 
 o_b\\ 
 o_c 
\end{pmatrix} 
\]
1. Check if $n_{a,b,c}$ is zero (MUST, otherwise you get \emph{divide by zero}). For example, if $n_c$ is zero: 
\[ 
\left( \begin{array}{ccc|c}
 n_a^2 & n_a & 1 & o_a\\ 
 n_b^2 & n_b & 1 & o_b\\ 
 0 & 0 & 1 & o_c
\end{array} \right) \rightarrow
\left( \begin{array}{ccc|c}
 n_a^2 & n_a & 1 & o_a\\ 
 0 & \underbrace{n_b n_a^2 - n_b^2 n_a}_{m} & \underbrace{n_a^2-n_b^2}_{n} & \underbrace{o_b n_a^2 - o_a n_b^2}_o\\ 
 0 & 0 & 1 & o_c
\end{array} \right)
\]
Therefore the solution is (same for if $n_{b,c}=0$ just changing the corresponding indices)
\[ 
\begin{array}{l}
 p_3 = o_a\\ 
 p_2 = \frac{1}{m}(o - np_3)\\ 
 p_1 = \frac{1}{n_a^2} (o_a - n_a p_2 - p_3).
\end{array}
\]
2. If there are no zero elements solve the whole equation system.
\[ 
\left( \begin{array}{ccc|c}
 n_a^2 & n_a & 1 & o_a\\ 
 0 & \underbrace{n_b n_a^2 - n_b^2 n_a}_{m} & \underbrace{n_a^2-n_b^2}_{n} & \underbrace{o_b n_a^2 - o_a n_b^2}_o\\ 
 0 & \underbrace{n_c n_a^2 - n_c^2 n_a}_{p} & \underbrace{n_a^2-n_c^2}_{q} & \underbrace{o_c n_a^2 - o_a n_c^2}_r\\ 
\end{array} \right)
\]
And the solution is
\[ 
\begin{array}{l}
 p_3 = \frac{rm-op}{qm-np}\\ 
 p_2 = \frac{1}{m}(o - np_3)\\ 
 p_1 = \frac{1}{n_a^2} (o_a - n_a p_2 - p_3).
\end{array}
\]
If the new calculated minima is a valid one (constraints are fullfilled) then 
we store the new minima at $n_c$ and keep the other 2 best solutions in $n_a$ and $n_b$ and 
repeat the calculation.
We stop, if : 1. constraints are no longer fullfilled $\rightarrow$ return last valid solution; 2. objective function value of the new minima is only slighty different to the old one (within $10^{-3}$--range); or 3. maximum number of iteration is exceeded.\\

Attention: the objective function is a non-smooth function which we try to locally approximate with a smooth parabola function. 



\section{Unsorted Ideas}
\subsection{Getting Rid of the PLiFs and Monotonicity Constraints}
Use logistic function $f(x|\beta,o,a) = \frac{a}{1+exp(-\beta(x+o))} $ ($\beta$ is slope, $o$ the offset and $a$ the amplitude) instead of plifs. Advantage: just 3 parameters per plif replacement ($\beta$, $a$ and $o$). Disadvantage: non-linear optimization problem. Example: 2 states, 10 signals and 20 plif points (limits must be placed in advance, too!) means 400 parameters to learn, in opposite 2 states, 10 signals and 3 variables means only 60 parameters to estimate (15\%)!



\end{document}

